# -*- coding: utf-8 -*-
"""semlab csa iris.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ug0z6SAM1OznS0oq98HbYgrdTDD9DHyR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score,classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier

df=pd.read_csv("/content/iris.csv")

"""preprocessing"""

# Display all columns and rows
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

data = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Handling negative values if any and preprocessing
df.dropna(axis=1, how='any', inplace=True)
df = df.loc[:, ~df.columns.duplicated()]
if df.isnull().values.any():
    df.fillna(df.median(), inplace=True)

"""visualizing outliers"""

plt.figure(figsize=(10, 10))
sns.boxplot(data)
plt.xlabel('Features')
plt.title('Boxplot to find outliers')

"""handling outliers"""

q1=data.quantile(0.25)
q3=data.quantile(0.75)
iqr=q3-q1
lb=q1-1.5*iqr
ub=q3+1.5*iqr

o=((data<lb)|(data>ub)).any(axis=1)
print('before imputing \n',data[o])
for col in data:
    data.loc[o,col]=data[col].mean()

print('after imputing \n',data[o])

print(df)
plt.figure(figsize=(10,10))
sns.boxplot(data)
plt.title('box plot after imputation')
plt.show()

plt.figure(figsize=(10,10))
sns.heatmap(data.corr(),annot=True,cmap='coolwarm')
plt.title('heatmap for visualization')
plt.show()

"""splitting data"""

# Prepare the data
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""adaboost"""

# Train the Adaboost classifier with default parameters
clf = AdaBoostClassifier(random_state=42)
clf.fit(X_train, y_train)
y_pred_adaboost = clf.predict(X_test)
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
print("Accuracy with default Adaboost classifier:", accuracy_adaboost)
print("Classification Report for Adaboost:")
print(classification_report(y_test, y_pred_adaboost))

"""1"""

# Adaboost with DecisionTreeClassifier as base learner
base_learner = DecisionTreeClassifier(max_depth=1)
clf = AdaBoostClassifier(estimator=base_learner, random_state=42)
clf.fit(X_train, y_train)
y_pred_dt_adaboost = clf.predict(X_test)
accuracy_dt_adaboost = accuracy_score(y_test, y_pred_dt_adaboost)
print("Accuracy with DecisionTreeClassifier as base learner:", accuracy_dt_adaboost)
print("Classification Report for Adaboost with DecisionTreeClassifier:")
print(classification_report(y_test, y_pred_dt_adaboost))

"""2"""

# Adaboost with RandomForestClassifier as base learner
base_learner = RandomForestClassifier(n_estimators=100, random_state=42)
clf = AdaBoostClassifier(estimator=base_learner, random_state=42)
clf.fit(X_train, y_train)
y_pred_rf_adaboost = clf.predict(X_test)
accuracy_rf_adaboost = accuracy_score(y_test, y_pred_rf_adaboost)
print("Accuracy with RandomForestClassifier as base learner:", accuracy_rf_adaboost)
print("Classification Report for Adaboost with RandomForestClassifier:")
print(classification_report(y_test, y_pred_rf_adaboost))

"""3"""

# Adaboost with Support Vector Machine (SVM) as base learner
base_learner = SVC(probability=True, kernel='linear')
clf = AdaBoostClassifier(estimator=base_learner, random_state=42, algorithm='SAMME')
clf.fit(X_train, y_train)
y_pred_svm_adaboost = clf.predict(X_test)
accuracy_svm_adaboost = accuracy_score(y_test, y_pred_svm_adaboost)
print("Accuracy with Support Vector Machine as base learner:", accuracy_svm_adaboost)
print("Classification Report for Adaboost with Support Vector Machine:")
print(classification_report(y_test, y_pred_svm_adaboost))

"""optimized code by writing a function , whichever works best for you when memorizing meaning cells 1,2,3 need not be written seperately"""

def train_and_evaluate_adaboost(base_learner, base_learner_name, X_train, y_train, X_test, y_test):
    clf = AdaBoostClassifier(estimator=base_learner, random_state=42)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy with", base_learner_name, "as base learner:", accuracy)
    print("Classification Report for Adaboost with", base_learner_name, ":")
    print(classification_report(y_test, y_pred))
    return y_pred

# Adaboost with DecisionTreeClassifier as base learner
y_pred_dt_adaboost = train_and_evaluate_adaboost(DecisionTreeClassifier(max_depth=1), "DecisionTreeClassifier", X_train, y_train, X_test, y_test)

# Adaboost with RandomForestClassifier as base learner
y_pred_rf_adaboost = train_and_evaluate_adaboost(RandomForestClassifier(n_estimators=100, random_state=42), "RandomForestClassifier", X_train, y_train, X_test, y_test)

# Adaboost with Support Vector Machine (SVM) as base learner
y_pred_svm_adaboost = train_and_evaluate_adaboost(SVC(probability=True, kernel='linear'), "Support Vector Machine", X_train, y_train, X_test, y_test)

"""varying no of estimators"""

n_estimators_values = [50, 100, 150, 200]

# Loop through each value of n_estimators
for n_estimators in n_estimators_values:
    # Train the Adaboost classifier with the current value of n_estimators
    clf = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)
    clf.fit(X_train, y_train)

    # Evaluate accuracy on the test set
    accuracy = accuracy_score(y_test, clf.predict(X_test))
    print("Accuracy with n_estimators =", n_estimators, ":", accuracy)

"""voting classifier"""

from sklearn.metrics import classification_report
from sklearn.ensemble import VotingClassifier
base_classifiers = [
    ('dt', DecisionTreeClassifier(max_depth=1)),
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('svm', SVC(probability=True, kernel='linear'))
]
voting_clf = VotingClassifier(estimators=base_classifiers, voting='hard')
voting_clf.fit(X_train, y_train)
y_pred_voting = voting_clf.predict(X_test)
accuracy_voting = accuracy_score(y_test, y_pred_voting)
print("Accuracy with Voting Classifier (Adaboost):", accuracy_voting)
print("Classification Report for Voting Classifier (Adaboost):")
print(classification_report(y_test, y_pred_voting))